---
title: "MBViz demo"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MBViz demo}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(magrittr)
library(MBViz)
library(ade4)
library(adegraphics)
library(ggplot2)
theme_set(theme_light())
```

## MB-PLS (multiblock PLS)
 
### On simulated data

```{r}

n <- 200 # number of observations
p <- 3:5 # number of variables in each X block
q <- 5 # number of variables in Y
Xs <- matrix(rnorm(n * sum(p)), nrow = n, ncol = sum(p)) |> as_blocks(p = p)
Y <- matrix(rnorm(n * q), nrow = n, ncol = q) %>% list(Y = .)


plot_mtb_blocks(Xs = Xs, Y = Y)
plot_mtb_data(Xs = Xs, Y = Y)

```


```{r}

PLS_A <- mbpls(
  dudiY = dudi.pca(df = Y$Y, scannf = FALSE, nf = 2),
  ktabX = ktab.list.df(list(X = Xs %>% blocks_to_df())),
  scale = TRUE,
  scannf = FALSE, nf = 2
)

plot_mtb(PLS_A)

```

```{r}

plot_mtb_bipc(res = PLS_A)
plot_mtb_vipc(res = PLS_A)


```



```{r}

plot_mtb_coef(res = PLS_A)

```



**Predictions**

To demonstrate the `mtb_predict` function, we build $Y$ as linear combinations of the $Xs$ so we can have perfect predictions.

```{r}

coefs <- matrix(rbinom(sum(p)*q, 1, prob = 0.5), sum(p), q)
Y <- (as.matrix(blocks_to_df(Xs)) %*% coefs) %>% list(Y = .)
ktabXs <- ktab.list.df(list(X = Xs %>% blocks_to_df()))

i_train <- sample(c(TRUE, FALSE), nrow(Xs$A), replace = TRUE, prob = c(0.8, 0.2))
j_train <- which(i_train); j_test <- which(!i_train)
ktabXs_train <- ktab.list.df(list(X = blocks_to_df(Xs)[j_train,] %>% set_rownames(1:sum(i_train))))
Y_train <- list(Y = Y$Y[j_train,] %>% set_rownames(1:sum(i_train)))
ktabXs_test <- ktab.list.df(list(X = blocks_to_df(Xs)[j_test,]))
Y_test <- list(Y = Y$Y[j_test,])


PLS_B <- 
  mbpls(
  dudiY = dudi.pca(df = Y_train$Y, scannf = FALSE, nf = 2),
  ktabX = ktabXs_train,
  scale = TRUE, option = "uniform",
  scannf = FALSE, nf = 12
)

plot(coefs, get_mtb_coef(res = PLS_B, as_matrix = TRUE), cex = 0.5, pch = 16); abline(a = 0, b = 1)

Y_test_hat <- 
  mtb_predict(
    res = PLS_B, original_Y = Y_train$Y, 
    newdata = ktabXs_test, original_X = ktabXs_train
    )
plot(Y_test_hat, Y_test$Y, pch = 16, cex = 0.5); abline(a = 0, b = 1)



```



```{r}

Y_hat <- 
  mtb_predict(
    res = res_mbplsda_success_endpoints_V4_Pl$res,
    original_Y = res_mbplsda_success_endpoints_V4_Pl$output,
    original_X = res_mbplsda_success_endpoints_V4_Pl$inputs |> ade4::ktab.list.df(),
    newdata = res_mbplsda_success_endpoints_V4_LV$inputs |> ade4::ktab.list.df()
    ) 

```





### On the `chicken` data from the `ade4` package

```{r}

data("chickenk")

```

```{r}
Y <- chickenk[1] # Mortality
Xs <- chickenk[2:5]
plot_mtb_blocks(Xs = Xs , Y = Y)
```

```{r}
plot_mtb_data(Xs = Xs, Y = Y)
```

```{r}

res <- 
  mbpls(
    dudiY = dudi.pca(Y$Mortality, scannf = FALSE, nf = 2),
    ktabX = ktab.list.df(Xs),
    option = "none",
    scale = TRUE,
    scannf = FALSE, nf = 4
    )

```


We can plot the results with the `adegraphics` `plot` function:

```{r}

plot(res)

```

Or use the `MBViz` functions that allow customization.

```{r}

plot_mtb_eig(res, xaxis = 1, yaxis = 2)

```

```{r}

plot_mtb_lX(res) 
# plot_mtb_lX(res, samples_color = Y$Mortality/rowSums(Y$Mortality)
# plot_mtb_lX(res, samples_color = sample(c(1,2), size = res$lw %>% sum(), replace = TRUE))

```

```{r}

plot_mtb_lY(res)
# plot_mtb_Y(res, samples_color = Y$Mortality/rowSums(Y$Mortality))
# BUGGY plot_mtb_Y(res, samples_color = sample(c(1,2), size = res$lw %>% sum(), replace = TRUE))

```

```{r}

plot_mtb_cov(res)

```


```{r}

plot_mtb_Xloadings(res)

```

```{r}

plot_mtb(res)

```


```{r}

plot_varX_varY(res = res, varX = "NbChick", varY = "Condemn")

```





```{r}


set.seed(123456)
test_res <- testdim(res, nrepet = 199, optdim = 4)
boot <- randboot(res, nrepet = 100, optdim = res$nf)


```

```{r}

plot_mtb_vipc(res = res, boot = boot)
plot_mtb_bipc(res = res, boot = boot)
plot_mtb_coef(res = res, boot = boot, Y_var = 1)

```


## MB-PLS-DA (multi-block PLS discriminant analysis)


```{r}

library(packMBPLSDA)

```

### On simulated data

We simulate some data:

```{r}

set.seed(123)
A <- simulate_Xs_Y(categorical_Y = TRUE, snr = 2)
visualize_simulated_latent_structure(A)

```

We fit the MBPLSDA on the simulated data

```{r}

res <- 
  mbplsda(
  dudiY = dudi.pca(A$Y, scannf = FALSE, nf = 3), 
  ktabX = ktab.list.df(A$Xs),
  scale = TRUE, option = "uniform", 
  scannf = FALSE, nf = ncol(A$Y)
  )

```


```{r}

plot_mtb(res, samples_color = A$Y)

```



We bootstrap confidence intervals

```{r}
boot <- boot_mbplsda(res, rep = 200, optdim = 2)
```



```{r}

plot_mtbda_coef(res = res, boot = boot, format = "wide")

```


```{r}

plot_mtb_bipc(res, boot = boot)
plot_mtb_vipc(res, boot = boot)

```

